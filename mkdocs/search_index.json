{
    "docs": [
        {
            "location": "/", 
            "text": "Welcome to Reed Haw's Open Research Notebook!\n\n\nThis website is an open workbook for a Carleton humanities course: \nCrafting Digital History\n. This course focuses on historical data and representation through digital means, as well as using digital technologies to clean, organize, analyze, and represent macrodata (or microdata in a very intricate way).\n\n\nView my GitHub Repository for Digital History \nHere\n\n\nI will be uploading my files for this course into this repository. Check back often to see what's new!\n\n\nThis course is actually all open source. If you want to follow along you can! Because \nCrafting Digital History\n is open source, and you can do all the same exercises that I do, and access all the same resources. If you read something in one of my \nLaboratory\n posts that intrigues you, head over to the \nCrafting Digital History workbook\n, and follow the modules from there! Even if it is just to find out how to install wget or to learn more interesting things like how Markdown works, or even to follow the MkDocs guide that helped me build this notebook! This is all thanks to \nDoctor Shawn Graham\n, who made it all happen.\n\n\nThese sites are still under construction.\n\n\nIt should be noted that both this site, and my other site (\nReed Haw.ca\n) are both under construction still. If you have problems viewing a page or if you notice something wrong, please feel free to \ncontact me\n! I appreciate all feedback. \n\n\nRead up on my work in each module!\n\n\nEvery module written on this website has a corresponding blog post, you can read about it at \nreedhaw.ca/cms/crafting-digital-history\n, and if you want to see the original exercises, you can follow each module heading in the workbook at \nworkbook.craftingdigitalhistory.ca\n.", 
            "title": "Home"
        }, 
        {
            "location": "/#welcome-to-reed-haws-open-research-notebook", 
            "text": "This website is an open workbook for a Carleton humanities course:  Crafting Digital History . This course focuses on historical data and representation through digital means, as well as using digital technologies to clean, organize, analyze, and represent macrodata (or microdata in a very intricate way).", 
            "title": "Welcome to Reed Haw's Open Research Notebook!"
        }, 
        {
            "location": "/#view-my-github-repository-for-digital-history-here", 
            "text": "I will be uploading my files for this course into this repository. Check back often to see what's new!  This course is actually all open source. If you want to follow along you can! Because  Crafting Digital History  is open source, and you can do all the same exercises that I do, and access all the same resources. If you read something in one of my  Laboratory  posts that intrigues you, head over to the  Crafting Digital History workbook , and follow the modules from there! Even if it is just to find out how to install wget or to learn more interesting things like how Markdown works, or even to follow the MkDocs guide that helped me build this notebook! This is all thanks to  Doctor Shawn Graham , who made it all happen.", 
            "title": "View my GitHub Repository for Digital History Here"
        }, 
        {
            "location": "/#these-sites-are-still-under-construction", 
            "text": "It should be noted that both this site, and my other site ( Reed Haw.ca ) are both under construction still. If you have problems viewing a page or if you notice something wrong, please feel free to  contact me ! I appreciate all feedback.", 
            "title": "These sites are still under construction."
        }, 
        {
            "location": "/#read-up-on-my-work-in-each-module", 
            "text": "Every module written on this website has a corresponding blog post, you can read about it at  reedhaw.ca/cms/crafting-digital-history , and if you want to see the original exercises, you can follow each module heading in the workbook at  workbook.craftingdigitalhistory.ca .", 
            "title": "Read up on my work in each module!"
        }, 
        {
            "location": "/hello_world/", 
            "text": "Hello World! (And a brief introduction to me!)\n\n\nHello there! My name is Reed, and I am currently working on a markdown document for the Digital Humanities course at Carleton. The reason I am taking this class is interesting, as it kind of correlates to my direct situation. I am majoring in history at Carleton, currently in my third year, but I think I am going to change my major to one in \nInteractive Multimedia and Design\n. When I found this course I thought it directly correlated to my situation, as I love history, and hope to find a way to intertwine history and technology, much as this course has. This is a terribly big decision for me, but I hope that it all works out in the end. I'm hoping that this gives me an idea of the things I could do if I switched, and the types of things I'd be facing. \n\n\n\n\nThis is one of the biggest decisions in my life, but I truley hope that it is something that I can excell at. I have previously made my own \nwebsite portfolio\n, which showcases some of my accomplishments in the tech world (mostly image editing). I have also taken an \nintroductory computer course\n at Carleton that really opened my eyes to how much I love technology (and yes, admittedly it was a easy, entry level course, but I rocked it). I'm hoping that I can get more invovled in technology here and discover the inner workings of more sophisticated technology. \n\n\n\n\nIn the end, I hope to leave this course with a significantly better understanding of how I can help combine history with technology, whether that is looking at big data or algorithms that can scan archives, I wish no more than to help people see history in a new light. I also hope I learn more about developing webpages and how to display information in a way that is helpful to people, and easy for them to understand (without making it ugly, \nugly websites are the worst\n).", 
            "title": "Hello world"
        }, 
        {
            "location": "/hello_world/#hello-world-and-a-brief-introduction-to-me", 
            "text": "Hello there! My name is Reed, and I am currently working on a markdown document for the Digital Humanities course at Carleton. The reason I am taking this class is interesting, as it kind of correlates to my direct situation. I am majoring in history at Carleton, currently in my third year, but I think I am going to change my major to one in  Interactive Multimedia and Design . When I found this course I thought it directly correlated to my situation, as I love history, and hope to find a way to intertwine history and technology, much as this course has. This is a terribly big decision for me, but I hope that it all works out in the end. I'm hoping that this gives me an idea of the things I could do if I switched, and the types of things I'd be facing.    This is one of the biggest decisions in my life, but I truley hope that it is something that I can excell at. I have previously made my own  website portfolio , which showcases some of my accomplishments in the tech world (mostly image editing). I have also taken an  introductory computer course  at Carleton that really opened my eyes to how much I love technology (and yes, admittedly it was a easy, entry level course, but I rocked it). I'm hoping that I can get more invovled in technology here and discover the inner workings of more sophisticated technology.    In the end, I hope to leave this course with a significantly better understanding of how I can help combine history with technology, whether that is looking at big data or algorithms that can scan archives, I wish no more than to help people see history in a new light. I also hope I learn more about developing webpages and how to display information in a way that is helpful to people, and easy for them to understand (without making it ugly,  ugly websites are the worst ).", 
            "title": "Hello World! (And a brief introduction to me!)"
        }, 
        {
            "location": "/markdown_cheetsheet/", 
            "text": "Copied from [https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet]\n\n\nHeaders\n\n\nH1\n\n\nH2\n\n\nH3\n\n\nH4\n\n\nH5\n\n\nH6\n\n\nAlternatively, for H1 and H2, an underline-ish style:\n\n\nAlt-H1\n\n\nAlt-H2\n\n\nH1\nH2\n\n\nH3\n\n\nH4\n\n\nH5\n\n\nH6\n\n\nAlternatively, for H1 and H2, an underline-ish style:\n\n\nAlt-H1\nAlt-H2\n\n\nEmphasis\n\n\nEmphasis, aka italics, with \nasterisks\n or \nunderscores\n.\n\n\nStrong emphasis, aka bold, with \nasterisks\n or \nunderscores\n.\n\n\nCombined emphasis with \nasterisks and \nunderscores\n.\n\n\nStrikethrough uses two tildes. ~~Scratch this.~~\nEmphasis, aka italics, with asterisks or underscores.\n\n\nStrong emphasis, aka bold, with asterisks or underscores.\n\n\nCombined emphasis with asterisks and underscores.\n\n\nStrikethrough uses two tildes. Scratch this.\n\n\nLists\n\n\n(In this example, leading and trailing spaces are shown with with dots: \u22c5)\n\n\n\n\nFirst ordered list item\n\n\nAnother item\n\u22c5\u22c5* Unordered sub-list. \n\n\nActual numbers don't matter, just that it's a number\n\u22c5\u22c51. Ordered sub-list\n\n\nAnd another item.\n\n\n\n\n\u22c5\u22c5\u22c5You can have properly indented paragraphs within list items. Notice the blank line above, and the leading spaces (at least one, but we'll use three here to also align the raw Markdown).\n\n\n\u22c5\u22c5\u22c5To have a line break without a paragraph, you will need to use two trailing spaces.\u22c5\u22c5\n\u22c5\u22c5\u22c5Note that this line is separate, but within the same paragraph.\u22c5\u22c5\n\u22c5\u22c5\u22c5(This is contrary to the typical GFM line break behaviour, where trailing spaces are not required.)\n\n\n\n\nUnordered list can use asterisks\n\n\nOr minuses\n\n\nOr pluses\nFirst ordered list item\nAnother item\nUnordered sub-list.\nActual numbers don't matter, just that it's a number\nOrdered sub-list\nAnd another item.\n\n\n\n\nYou can have properly indented paragraphs within list items. Notice the blank line above, and the leading spaces (at least one, but we'll use three here to also align the raw Markdown).\n\n\nTo have a line break without a paragraph, you will need to use two trailing spaces.\nNote that this line is separate, but within the same paragraph.\n(This is contrary to the typical GFM line break behaviour, where trailing spaces are not required.)\n\n\nUnordered list can use asterisks\nOr minuses\nOr pluses\n\n\nLinks\n\n\nThere are two ways to create links.\n\n\nI'm an inline-style link\n\n\nI'm an inline-style link with title\n\n\nI'm a reference-style link\n\n\nI'm a relative reference to a repository file\n\n\nYou can use numbers for reference-style link definitions\n\n\nOr leave it empty and use the \nlink text itself\n.\n\n\nURLs and URLs in angle brackets will automatically get turned into links. \nhttp://www.example.com or \nhttp://www.example.com\n and sometimes \nexample.com (but not on Github, for example).\n\n\nSome text to show that the reference links can follow later.\n\n\nI'm an inline-style link\n\n\nI'm an inline-style link with title\n\n\nI'm a reference-style link\n\n\nI'm a relative reference to a repository file\n\n\nYou can use numbers for reference-style link definitions\n\n\nOr leave it empty and use the link text itself.\n\n\nURLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or http://www.example.com and sometimes example.com (but not on Github, for example).\n\n\nSome text to show that the reference links can follow later.\n\n\nImages\n\n\nHere's our logo (hover to see the title text):\n\n\nInline-style: \n\n\n\nReference-style: \n\n\n\nHere's our logo (hover to see the title text):\n\n\nInline-style:  alt text\n\n\nReference-style:  alt text\n\n\nCode and Syntax Highlighting\n\n\nCode blocks are part of the Markdown spec, but syntax highlighting isn't. However, many renderers -- like Github's and Markdown Here -- support syntax highlighting. Which languages are supported and how those language names should be written will vary from renderer to renderer. Markdown Here supports highlighting for dozens of languages (and not-really-languages, like diffs and HTTP headers); to see the complete list, and how to write the language names, see the highlight.js demo page.\n\n\nInline \ncode\n has \nback-ticks around\n it.\nInline code has back-ticks around it.\n\n\nBlocks of code are either fenced by lines with three back-ticks ```, or are indented with four spaces. I recommend only using the fenced code blocks -- they're easier and only they support syntax highlighting.\n\n\nvar s = \nJavaScript syntax highlighting\n;\nalert(s);\n\n\n\n\ns = \nPython syntax highlighting\n\nprint s\n\n\n\n\nNo language indicated, so no syntax highlighting. \nBut let's throw in a \nb\ntag\n/b\n.\n\n\n\n\nvar s = \"JavaScript syntax highlighting\";\nalert(s);\ns = \"Python syntax highlighting\"\nprint s\nNo language indicated, so no syntax highlighting in Markdown Here (varies on Github). \nBut let's throw in a \ntag\n.\n\n\nTables\n\n\nTables aren't part of the core Markdown spec, but they are part of GFM and Markdown Here supports them. They are an easy way of adding tables to your email -- a task that would otherwise require copy-pasting from another application.\n\n\nColons can be used to align columns.\n\n\n\n\n\n\n\n\nTables\n\n\nAre\n\n\nCool\n\n\n\n\n\n\n\n\n\n\ncol 3 is\n\n\nright-aligned\n\n\n$1600\n\n\n\n\n\n\ncol 2 is\n\n\ncentered\n\n\n$12\n\n\n\n\n\n\nzebra stripes\n\n\nare neat\n\n\n$1\n\n\n\n\n\n\n\n\nThere must be at least 3 dashes separating each header cell.\nThe outer pipes (|) are optional, and you don't need to make the \nraw Markdown line up prettily. You can also use inline Markdown.\n\n\n\n\n\n\n\n\nMarkdown\n\n\nLess\n\n\nPretty\n\n\n\n\n\n\n\n\n\n\nStill\n\n\nrenders\n\n\nnicely\n\n\n\n\n\n\n1\n\n\n2\n\n\n3\n\n\n\n\n\n\nColons can be used to align columns.\n\n\n\n\n\n\n\n\n\n\n\n\nTables  Are Cool\ncol 3 is    right-aligned   $1600\ncol 2 is    centered    $12\nzebra stripes   are neat    $1\nThere must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don't need to make the raw Markdown line up prettily. You can also use inline Markdown.\n\n\nMarkdown    Less    Pretty\nStill   renders nicely\n1   2   3\n\n\nBlockquotes\n\n\n\n\nBlockquotes are very handy in email to emulate reply text.\nThis line is part of the same quote.\n\n\n\n\nQuote break.\n\n\n\n\nThis is a very long line that will still be quoted properly when it wraps. Oh boy let's keep writing to make sure this is long enough to actually wrap for everyone. Oh, you can \nput\n \nMarkdown\n into a blockquote. \nBlockquotes are very handy in email to emulate reply text. This line is part of the same quote.\nQuote break.\n\n\n\n\nThis is a very long line that will still be quoted properly when it wraps. Oh boy let's keep writing to make sure this is long enough to actually wrap for everyone. Oh, you can put Markdown into a blockquote.\n\n\nInline HTML\n\n\nYou can also use raw HTML in your Markdown, and it'll mostly work pretty well.\n\n\n\n  \nDefinition list\n\n  \nIs something people use sometimes.\n\n\n  \nMarkdown in HTML\n\n  \nDoes *not* work **very** well. Use HTML \ntags\n.\n\n\n\n\n\nDefinition list\nIs something people use sometimes.\nMarkdown in HTML\nDoes \nnot\n work \nvery\n well. Use HTML tags.\n\n\nHorizontal Rule\n\n\nThree or more...\n\n\n\n\nHyphens\n\n\n\n\nAsterisks\n\n\n\n\nUnderscores\nThree or more...\n\n\nHyphens\n\n\nAsterisks\n\n\nUnderscores\n\n\nLine Breaks\n\n\nMy basic recommendation for learning how line breaks work is to experiment and discover -- hit \n once (i.e., insert one newline), then hit it twice (i.e., insert two newlines), see what happens. You'll soon learn to get what you want. \"Markdown Toggle\" is your friend.\n\n\nHere are some things to try out:\n\n\nHere's a line for us to start with.\n\n\nThis line is separated from the one above by two newlines, so it will be a \nseparate paragraph\n.\n\n\nThis line is also a separate paragraph, but...\nThis line is only separated by a single newline, so it's a separate line in the \nsame paragraph\n.\nHere's a line for us to start with.\n\n\nThis line is separated from the one above by two newlines, so it will be a separate paragraph.\n\n\nThis line is also begins a separate paragraph, but...\nThis line is only separated by a single newline, so it's a separate line in the same paragraph.\n\n\n(Technical note: Markdown Here uses GFM line breaks, so there's no need to use MD's two-space line breaks.)\n\n\nYoutube videos\n\n\nThey can't be added directly but you can add an image with a link to the video like this:\n\n\n\nOr, in pure Markdown, but losing the image sizing and border:\n\n\n\nReferencing a bug by #bugID in your git commit links it to the slip. For example #1.\nStatus API Training Shop Blog About Pricing\n\u00a9 2016 GitHub, Inc. Terms Privacy Security Contact Help", 
            "title": "Markdown cheetsheet"
        }, 
        {
            "location": "/markdown_cheetsheet/#h1", 
            "text": "", 
            "title": "H1"
        }, 
        {
            "location": "/markdown_cheetsheet/#h2", 
            "text": "", 
            "title": "H2"
        }, 
        {
            "location": "/markdown_cheetsheet/#h3", 
            "text": "", 
            "title": "H3"
        }, 
        {
            "location": "/markdown_cheetsheet/#h4", 
            "text": "", 
            "title": "H4"
        }, 
        {
            "location": "/markdown_cheetsheet/#h5", 
            "text": "", 
            "title": "H5"
        }, 
        {
            "location": "/markdown_cheetsheet/#h6", 
            "text": "Alternatively, for H1 and H2, an underline-ish style:", 
            "title": "H6"
        }, 
        {
            "location": "/markdown_cheetsheet/#alt-h1", 
            "text": "", 
            "title": "Alt-H1"
        }, 
        {
            "location": "/markdown_cheetsheet/#alt-h2", 
            "text": "H1\nH2  H3  H4  H5  H6  Alternatively, for H1 and H2, an underline-ish style:  Alt-H1\nAlt-H2  Emphasis  Emphasis, aka italics, with  asterisks  or  underscores .  Strong emphasis, aka bold, with  asterisks  or  underscores .  Combined emphasis with  asterisks and  underscores .  Strikethrough uses two tildes. ~~Scratch this.~~\nEmphasis, aka italics, with asterisks or underscores.  Strong emphasis, aka bold, with asterisks or underscores.  Combined emphasis with asterisks and underscores.  Strikethrough uses two tildes. Scratch this.  Lists  (In this example, leading and trailing spaces are shown with with dots: \u22c5)   First ordered list item  Another item\n\u22c5\u22c5* Unordered sub-list.   Actual numbers don't matter, just that it's a number\n\u22c5\u22c51. Ordered sub-list  And another item.   \u22c5\u22c5\u22c5You can have properly indented paragraphs within list items. Notice the blank line above, and the leading spaces (at least one, but we'll use three here to also align the raw Markdown).  \u22c5\u22c5\u22c5To have a line break without a paragraph, you will need to use two trailing spaces.\u22c5\u22c5\n\u22c5\u22c5\u22c5Note that this line is separate, but within the same paragraph.\u22c5\u22c5\n\u22c5\u22c5\u22c5(This is contrary to the typical GFM line break behaviour, where trailing spaces are not required.)   Unordered list can use asterisks  Or minuses  Or pluses\nFirst ordered list item\nAnother item\nUnordered sub-list.\nActual numbers don't matter, just that it's a number\nOrdered sub-list\nAnd another item.   You can have properly indented paragraphs within list items. Notice the blank line above, and the leading spaces (at least one, but we'll use three here to also align the raw Markdown).  To have a line break without a paragraph, you will need to use two trailing spaces.\nNote that this line is separate, but within the same paragraph.\n(This is contrary to the typical GFM line break behaviour, where trailing spaces are not required.)  Unordered list can use asterisks\nOr minuses\nOr pluses  Links  There are two ways to create links.  I'm an inline-style link  I'm an inline-style link with title  I'm a reference-style link  I'm a relative reference to a repository file  You can use numbers for reference-style link definitions  Or leave it empty and use the  link text itself .  URLs and URLs in angle brackets will automatically get turned into links. \nhttp://www.example.com or  http://www.example.com  and sometimes \nexample.com (but not on Github, for example).  Some text to show that the reference links can follow later.  I'm an inline-style link  I'm an inline-style link with title  I'm a reference-style link  I'm a relative reference to a repository file  You can use numbers for reference-style link definitions  Or leave it empty and use the link text itself.  URLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or http://www.example.com and sometimes example.com (but not on Github, for example).  Some text to show that the reference links can follow later.  Images  Here's our logo (hover to see the title text):  Inline-style:   Reference-style:   Here's our logo (hover to see the title text):  Inline-style:  alt text  Reference-style:  alt text  Code and Syntax Highlighting  Code blocks are part of the Markdown spec, but syntax highlighting isn't. However, many renderers -- like Github's and Markdown Here -- support syntax highlighting. Which languages are supported and how those language names should be written will vary from renderer to renderer. Markdown Here supports highlighting for dozens of languages (and not-really-languages, like diffs and HTTP headers); to see the complete list, and how to write the language names, see the highlight.js demo page.  Inline  code  has  back-ticks around  it.\nInline code has back-ticks around it.  Blocks of code are either fenced by lines with three back-ticks ```, or are indented with four spaces. I recommend only using the fenced code blocks -- they're easier and only they support syntax highlighting.  var s =  JavaScript syntax highlighting ;\nalert(s);  s =  Python syntax highlighting \nprint s  No language indicated, so no syntax highlighting. \nBut let's throw in a  b tag /b .  var s = \"JavaScript syntax highlighting\";\nalert(s);\ns = \"Python syntax highlighting\"\nprint s\nNo language indicated, so no syntax highlighting in Markdown Here (varies on Github). \nBut let's throw in a  tag .  Tables  Tables aren't part of the core Markdown spec, but they are part of GFM and Markdown Here supports them. They are an easy way of adding tables to your email -- a task that would otherwise require copy-pasting from another application.  Colons can be used to align columns.     Tables  Are  Cool      col 3 is  right-aligned  $1600    col 2 is  centered  $12    zebra stripes  are neat  $1     There must be at least 3 dashes separating each header cell.\nThe outer pipes (|) are optional, and you don't need to make the \nraw Markdown line up prettily. You can also use inline Markdown.     Markdown  Less  Pretty      Still  renders  nicely    1  2  3    Colons can be used to align columns.       Tables  Are Cool\ncol 3 is    right-aligned   $1600\ncol 2 is    centered    $12\nzebra stripes   are neat    $1\nThere must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don't need to make the raw Markdown line up prettily. You can also use inline Markdown.  Markdown    Less    Pretty\nStill   renders nicely\n1   2   3  Blockquotes   Blockquotes are very handy in email to emulate reply text.\nThis line is part of the same quote.   Quote break.   This is a very long line that will still be quoted properly when it wraps. Oh boy let's keep writing to make sure this is long enough to actually wrap for everyone. Oh, you can  put   Markdown  into a blockquote. \nBlockquotes are very handy in email to emulate reply text. This line is part of the same quote.\nQuote break.   This is a very long line that will still be quoted properly when it wraps. Oh boy let's keep writing to make sure this is long enough to actually wrap for everyone. Oh, you can put Markdown into a blockquote.  Inline HTML  You can also use raw HTML in your Markdown, and it'll mostly work pretty well.  \n   Definition list \n   Is something people use sometimes. \n\n   Markdown in HTML \n   Does *not* work **very** well. Use HTML  tags .   Definition list\nIs something people use sometimes.\nMarkdown in HTML\nDoes  not  work  very  well. Use HTML tags.  Horizontal Rule  Three or more...   Hyphens   Asterisks   Underscores\nThree or more...  Hyphens  Asterisks  Underscores  Line Breaks  My basic recommendation for learning how line breaks work is to experiment and discover -- hit   once (i.e., insert one newline), then hit it twice (i.e., insert two newlines), see what happens. You'll soon learn to get what you want. \"Markdown Toggle\" is your friend.  Here are some things to try out:  Here's a line for us to start with.  This line is separated from the one above by two newlines, so it will be a  separate paragraph .  This line is also a separate paragraph, but...\nThis line is only separated by a single newline, so it's a separate line in the  same paragraph .\nHere's a line for us to start with.  This line is separated from the one above by two newlines, so it will be a separate paragraph.  This line is also begins a separate paragraph, but...\nThis line is only separated by a single newline, so it's a separate line in the same paragraph.  (Technical note: Markdown Here uses GFM line breaks, so there's no need to use MD's two-space line breaks.)  Youtube videos  They can't be added directly but you can add an image with a link to the video like this:  \nOr, in pure Markdown, but losing the image sizing and border:  \nReferencing a bug by #bugID in your git commit links it to the slip. For example #1.\nStatus API Training Shop Blog About Pricing\n\u00a9 2016 GitHub, Inc. Terms Privacy Security Contact Help", 
            "title": "Alt-H2"
        }, 
        {
            "location": "/Module_2/", 
            "text": "Module 2\n\n\n2/16/2016 8:37:25 PM \n\n\nExercise 1\n\n\nI did a search for my surname (Haw) in the memorial database, and found no casualties reported in the war. Now I do know this is accurate (on the Canadian side) as all the members in my family that fought in the war survived. However, upon doing a search for Haw in England, I found four servicemen that died in the Wars, and they all appeared to be from the same family (John and Richard were both common names among the four). \n\n\nExercise 2\n\n\nUsing Outwit Hub I imported the contents of the search for \"pie\" on [http://www.stoa.org/]. Scraping the HTML from the search page allowed me to view the elements that are invisible behind the CSS of the page (mostly). I've added a few lines of the table below:\n\n\n\n\n\n\n\n\nCollection Time\n\n\nSource URL\n\n\nTranscription\n\n\n\n\n\n\n\n\n\n\n2/16/2016 6:26:16 PM\n\n\nhttp://www.stoa.org/sol-bin/search.pl\n\n\nMeaning you will depart. Sophocles [sc. uses the word].\n\n\n\n\n\n\n2/16/2016 6:26:16 PM\n\n\nhttp://www.stoa.org/sol-bin/search.pl\n\n\n[Meaning] to the ground, into/towards [the] earth. Also [sc. attested is] \u03c7\u03b1\u03bc\u1fb6\u03b6\u03b5, [also meaning] to the ground.\n\n\n\n\n\n\n2/16/2016 6:26:16 PM\n\n\nhttp://www.stoa.org/sol-bin/search.pl\n\n\n[The words] \u03c7\u03bb\u03bf\u03b5\u03c1\u1f79\u03c2 and \u03c7\u03bb\u03c9\u03c1\u1f79\u03c2 [\"pale green\"] [are derived] from \u03c7\u03bf\u03bb\u1f75 [\"bile\"], and from this the [word] \u1f60\u03c7\u03c1\u1f79\u03c2 [\"sallow\"] [also comes]. And [sc. also attested is] \u03c7\u03bb\u03c9\u03c1\u1f79\u03c4\u03b7\u03c2 [\"greenness\"].\n\n\n\n\n\n\n\n\nThis information can be valuable when looking at large allotments of information. A search can only give you one or two things at a time, but by using software such as Outwit Hub, I am able to capture multiple pieces of data at a time from a web page, including source URL, date and the captured text. By modifying the search parameters, one could easily use Outwit hub to quickly capture large amounts of search data, and by compiling into a spreadsheet it allows for  easy viewing and visualization of the important information, as well as the ability to create data charts and graphs. Ultimately it would be one of the easiest ways to gather information. The ability to see a compilation of what is written in the page gives a powerful overview, which is especially important in research. \n\n\nExercise 3\n\n\nThis section of the module was one I was nervous to begin. I was told that it was going to be a difficult section, especially for Windows users (which, of course, I am one). But entering this Exercise I didn't find it as difficult as I had anticipated. Most of the stuff that is typically unused by other people, but editing the \npath\n variable under system environments to include CoreUtils and installing wget through chocolatey are all things that I had previously had to deal with. My previous experience working with these advanced settings and programs made it significantly easier to work with the Canadiana JSON program. I used the search for articles and pieces on the Titanic written between 1910 and 1916, and with the program I was able to download these articles onto my computer. One thing I noticed is the first two tests I ran of the program it was able to recognize and work properly, but I was getting the error that wget was an \nuknown command\n, so I had to figure out how to properly install wget. I realized I could use chocolatey in either Git Bash or command prompt to download and install the program (chocolatey is now one of my favourite programs). After successfully installing this, I got my \noutput.txt\n file, and the only thing that remained was to split it up. I tried running \nsplitthingsup.sh\n but it gave my over 24,000 files that mostly had 1 kb of data for each containing next to nothing. So I tried the code \nawk '/identifier/{\"F\"++i;}{print \n \"newoutput\"i\".txt\";}' output.txt\n written by Lee Mordechai, and it worked significantly better, leaving me with 204 files. Much more manageable. I might even take a look to see if I can divide it up split it up with my own identifiers. \n\n\nExercise 4\n\n\nThis exercise was more of a developmental/eyeopening thing for most people I think. Backing up and redundancies is probably one of my favourite things to do. I am a firm believer that things need to be backed up on a regular basis. I have my phone backing up to a cloud daily, and my computer backing up to an external drive every hour. I hope to set up NAS (network attached storage) server using an old computer, because I'm weirdly paranoid about that. So reading Donna Yates redundancies made me excessively happy. Although I don't have three external hard drives, I love the way Yates does this backup, and all of the redundancies to ensure that they will never be lost. I can see the paranoia behind the failing URLs and the slight possibility of corrupting the PDFs, I would likely not have quite so many redundancies as that, but I also am not writing academic articles for my career, but if I was I would likely do the same. \n\n\nNow, if I were to do the same from now on, I would likely create a folder for the paper and save the PDFs there, and create a backup on my external. However, I have never used BibDesk before, but I am definitely going to try it, and if I like it I will definitely use it. And when it comes to me and filling in fields, I have to complete them all. \n\n\nGoing Further: Wget\n\n\nSo Wget is one of those things that I think would be very VERY useful. And I basically have a handle on it. I managed to get it to mostly work at downloading the archives.org articles, but my biggest problem was it was originally saying there was an error unable to resolve the host in regards to the options for wget, aka a \n-i\n or \n-e robots=off\n, so I went to the Wget cheat sheet and read through all of it. In it I was able to see what each command was and tried playing around with a few things. After a while I finally got the program to start grabbing things, but then I was getting \nError ... 404 not found\n, so upon inspection I noticed that Wget was adding %22 to each one of my URLs, so something was wrong there with my text file. Unfortunately I don't quite have time to look into it too deeply at the moment, but when I do have troubles with Wget in the future, I at least have an idea at what I need to look at to find the problem. It is definitely one of my favourite tools so far though.", 
            "title": "Module 2"
        }, 
        {
            "location": "/Module_2/#module-2", 
            "text": "2/16/2016 8:37:25 PM", 
            "title": "Module 2"
        }, 
        {
            "location": "/Module_2/#exercise-1", 
            "text": "I did a search for my surname (Haw) in the memorial database, and found no casualties reported in the war. Now I do know this is accurate (on the Canadian side) as all the members in my family that fought in the war survived. However, upon doing a search for Haw in England, I found four servicemen that died in the Wars, and they all appeared to be from the same family (John and Richard were both common names among the four).", 
            "title": "Exercise 1"
        }, 
        {
            "location": "/Module_2/#exercise-2", 
            "text": "Using Outwit Hub I imported the contents of the search for \"pie\" on [http://www.stoa.org/]. Scraping the HTML from the search page allowed me to view the elements that are invisible behind the CSS of the page (mostly). I've added a few lines of the table below:     Collection Time  Source URL  Transcription      2/16/2016 6:26:16 PM  http://www.stoa.org/sol-bin/search.pl  Meaning you will depart. Sophocles [sc. uses the word].    2/16/2016 6:26:16 PM  http://www.stoa.org/sol-bin/search.pl  [Meaning] to the ground, into/towards [the] earth. Also [sc. attested is] \u03c7\u03b1\u03bc\u1fb6\u03b6\u03b5, [also meaning] to the ground.    2/16/2016 6:26:16 PM  http://www.stoa.org/sol-bin/search.pl  [The words] \u03c7\u03bb\u03bf\u03b5\u03c1\u1f79\u03c2 and \u03c7\u03bb\u03c9\u03c1\u1f79\u03c2 [\"pale green\"] [are derived] from \u03c7\u03bf\u03bb\u1f75 [\"bile\"], and from this the [word] \u1f60\u03c7\u03c1\u1f79\u03c2 [\"sallow\"] [also comes]. And [sc. also attested is] \u03c7\u03bb\u03c9\u03c1\u1f79\u03c4\u03b7\u03c2 [\"greenness\"].     This information can be valuable when looking at large allotments of information. A search can only give you one or two things at a time, but by using software such as Outwit Hub, I am able to capture multiple pieces of data at a time from a web page, including source URL, date and the captured text. By modifying the search parameters, one could easily use Outwit hub to quickly capture large amounts of search data, and by compiling into a spreadsheet it allows for  easy viewing and visualization of the important information, as well as the ability to create data charts and graphs. Ultimately it would be one of the easiest ways to gather information. The ability to see a compilation of what is written in the page gives a powerful overview, which is especially important in research.", 
            "title": "Exercise 2"
        }, 
        {
            "location": "/Module_2/#exercise-3", 
            "text": "This section of the module was one I was nervous to begin. I was told that it was going to be a difficult section, especially for Windows users (which, of course, I am one). But entering this Exercise I didn't find it as difficult as I had anticipated. Most of the stuff that is typically unused by other people, but editing the  path  variable under system environments to include CoreUtils and installing wget through chocolatey are all things that I had previously had to deal with. My previous experience working with these advanced settings and programs made it significantly easier to work with the Canadiana JSON program. I used the search for articles and pieces on the Titanic written between 1910 and 1916, and with the program I was able to download these articles onto my computer. One thing I noticed is the first two tests I ran of the program it was able to recognize and work properly, but I was getting the error that wget was an  uknown command , so I had to figure out how to properly install wget. I realized I could use chocolatey in either Git Bash or command prompt to download and install the program (chocolatey is now one of my favourite programs). After successfully installing this, I got my  output.txt  file, and the only thing that remained was to split it up. I tried running  splitthingsup.sh  but it gave my over 24,000 files that mostly had 1 kb of data for each containing next to nothing. So I tried the code  awk '/identifier/{\"F\"++i;}{print   \"newoutput\"i\".txt\";}' output.txt  written by Lee Mordechai, and it worked significantly better, leaving me with 204 files. Much more manageable. I might even take a look to see if I can divide it up split it up with my own identifiers.", 
            "title": "Exercise 3"
        }, 
        {
            "location": "/Module_2/#exercise-4", 
            "text": "This exercise was more of a developmental/eyeopening thing for most people I think. Backing up and redundancies is probably one of my favourite things to do. I am a firm believer that things need to be backed up on a regular basis. I have my phone backing up to a cloud daily, and my computer backing up to an external drive every hour. I hope to set up NAS (network attached storage) server using an old computer, because I'm weirdly paranoid about that. So reading Donna Yates redundancies made me excessively happy. Although I don't have three external hard drives, I love the way Yates does this backup, and all of the redundancies to ensure that they will never be lost. I can see the paranoia behind the failing URLs and the slight possibility of corrupting the PDFs, I would likely not have quite so many redundancies as that, but I also am not writing academic articles for my career, but if I was I would likely do the same.   Now, if I were to do the same from now on, I would likely create a folder for the paper and save the PDFs there, and create a backup on my external. However, I have never used BibDesk before, but I am definitely going to try it, and if I like it I will definitely use it. And when it comes to me and filling in fields, I have to complete them all.", 
            "title": "Exercise 4"
        }, 
        {
            "location": "/Module_2/#going-further-wget", 
            "text": "So Wget is one of those things that I think would be very VERY useful. And I basically have a handle on it. I managed to get it to mostly work at downloading the archives.org articles, but my biggest problem was it was originally saying there was an error unable to resolve the host in regards to the options for wget, aka a  -i  or  -e robots=off , so I went to the Wget cheat sheet and read through all of it. In it I was able to see what each command was and tried playing around with a few things. After a while I finally got the program to start grabbing things, but then I was getting  Error ... 404 not found , so upon inspection I noticed that Wget was adding %22 to each one of my URLs, so something was wrong there with my text file. Unfortunately I don't quite have time to look into it too deeply at the moment, but when I do have troubles with Wget in the future, I at least have an idea at what I need to look at to find the problem. It is definitely one of my favourite tools so far though.", 
            "title": "Going Further: Wget"
        }, 
        {
            "location": "/Module_3/", 
            "text": "Module 3\n\n\n3/19/2016 1:16:47 PM \n\n\nText Encoding\n\n\nRecovered Histories\n Vetting\n\n\nNegatives:\n\n\n\n\nSite is \"Lottery Funded\", and does not appear to be affiliated with an academic institution\n\n\nWebsites design and style is outdated, which creates possibility that all the information is outdated, or not to modern standards \n\n\nDoes not have any outside links or sources, other than what is presented in its collection \n\n\nA significant portion of the literature in the collection appears to be from slavers and free white men\n\n\nSome items in collection have very messy handwriting, making it more difficult to encode the information and recognize characters \n\n\n\n\nPositives:\n\n\n\n\nSite is \"Lottery Funded\", which can be viewed in a positive way, as it prevents people from becoming biased or for altering information to help prove their point\n\n\nEncourages users to generate own content using site as raw resource\n\n\nAppears to reference information in its collection by linking to the specific articles\n\n\nDocuments in collection are scanned and then uploaded, they are not transcribed, which can cause differences\n\n\nSearch tool provides what appears to be accurate reading of the collection\n\n\n\n\nTranscription\n\n\nThe transcription went smoothly at first. Upon entering my first tags, I tested it in Internet Explorer and got the colourfully transcribed version of \nNegro Slavery by Zachary MacAulay\n. But then I went back and added some a few more place and name tags, and the text disappeared. I had the paragraphs separated and encased in tags, rather than inline tags, and I think that's where my problem came from. For example:\n\n\nAfter going through each paragraph break, I managed to get proper formatting back, with colours. My suspicion is that one of my location tags got separated when I was spacing out the text. A simple mistake.  \n\n\nTransformations\n\n\nIn my page5.xml file, the xsl styling is looking for tags like \npersName\n and \ninterp\n. These titles point the browser to the same tag in the .xsl document which defines the style of the .xml file. This uses the same system that web pages utilize, the HTML pointing the browser to the styles defined in the CSS document. \n\n\nIn \nSG_transformer.xsl\n, the browser will be looking at tags such as \nID\n, or \nRepository\n. I \nbelieve\n this uses an elif system, where it defines a list of words as the root of the tag, and each tag would receive a value-of, which might give a tooltip? I'm not 100% on the appearance of the .xml page once these tags are used. \n\n\nTo define different .xsl sheets, you would change the \nhref\n designation on line 2 from: \n?cml-stylesheet type=\"text/xsl\" href=\"000stylesheet.xsl\"?\n to \n?cml-stylesheet type=\"text/xsl\" href=\"SG_transformer.xsl\"?\n. I tried this and didn't get any change in my own .xml file, but I believe that is 1. because I don't have any of the tags and 2. because the \nSG_transformer.xsl\n file does not have any defined template styles in it. I compared it to the 000style.xsl and the colours and appearance of the tags are defined under the \nstyle\n tag throughout the document, which \nSG_transformer.xsl\n is lacking. \n\n\nIf you want to see the pages, you can view my transcription \nhere\n. I also apologize for putting the transformer.xsl file as code, the underscore in it was italicizing large swathes of my text that happened to fall between them. \n\n\nRegex\n\n\nAfter reading the gentle introduction, I can understand how powerful it can be once one knows how to use it. It's a good thing I made a copy. I also need to be careful as to where I leave my cursor when I'm searching. When I leave it at the end, it's hard to find anything in the document after it. \n\n\n\\r\\n\n did not work to find the blank lines. To remove the white spaces, I went into \nRegExr's Cheatsheet\n and found the Regex for word, which was \n\\w\n, and then used \n\\r\\n[^\\w].+\n to find all of the white spaces in my document to remove. It successfully worked, however it did leave a few large chunks with blank lines that I can't get RegEx to clear. I suspect that it is because there was more than one line with it, and when I replaced the previous characters with the the empty replace bar, it replaced everything, but maybe put in an invisible character I can't see. I'm not sure yet, still trying. For the sake of my sanity I had to go through and remove each chunk by hand. It wasn't difficult though, as there were only twelve or so of these chunks. I think it's where \"Digitized By Google\" used to sit in the document, and they may have put in something that my text editor can't see/copy. Still not sure.  \n\n\nI find that \nRegExr\n doesn't seem to process my work properly. I'm not sure why, but I actually find it relatively easy to use Notepad++ as it displays a window at the bottom of the document with all of the information that the find function works. And if something doesn't work, ctrl+Z works magical wonders. \n\n\nWhile trying to replace the page number and comma at the end of each index entry, I was getting very bizarre numbers at the end of the entry. Until I realized I had put the space in the wrong spot. When replacing \n(, )([0-9]{4})(.+)\n with \n\\2\n I was getting an entry that looked like this: \n\n~Sam Houston to J. Pinckney Henderson, December 311836\n\nThen I realized my misplaced space was in the wrong group, and found that \n(,)( [0-9]{4})(.+)\n worked significantly better. I proceeded to have a moment of panic when I noticed lines like this: \n\nDavid G. Bumet, Alc6e La Branche, December 16, 1839 61\nDavid G. Bumet, Alc^ La Branche, December 17, 1839 63\n\nBut then upon checking the original document, that is what Google's OCR thought the characters represented.\n\n\nSomething messed up my saves and I managed to save over my step 5 with an earlier version, so I have to backtrack a bit to get back on track, and redo step five and six. In the midst of finding this, I realized I had left the dates at the ends of each entry, so every search would come up with more than three commas in each sentence. So backtrack a little further to step 4. And I managed to fix it with the given Regex. I'm not a hundred percent on what made that occur. \n\n\nOpen Refine\n\n\nInteresting tool. I worked through it all only to find that it left a few entries (possibly worse off). I think its because I might have been trying to add an accented e, but it didn't like it, as a few of my entries have an @ sign in it randomly that wasn't there before. I might try running it through again later, but at least I now know how to basically work the program (which is awesome). Just to play with it, I'm now uploading my .csv into Palladio to see what it comes up with. Data visualization is one of my favourite things! \n\n\nI don't quite understand it at the moment. Maybe I'll come back to it. \n\n\nStanford NER\n\n\nSo I'm having significant issues with this one. I can't seem to run the \nner-gui.bat\n file to run, it opens a window only to immediately close it again. So following the directions on the end of the Open Refine page, I attempted to run it from the command line in the folder and I'm getting an exception in thread, but I can't figure out what I've done wrong. I have a feeling its something to do with a missing Java definition. I think it's specifically struggling with \nedu.stanford.nlp.ie.crf.CRFClassifier\n  \n\n\nI figured it out. The problem was that I've been trying to run it from command line with the text from the \nworkbook\n, which wasn't working. So I tried updating Java, and did that successfully. Still nothing. I was able to successfully open a gui using \nstanford-ner.jar\n, but when I put the text in and selected a classifier, it was not doing anything. So I deleted a large selection of it and finally got my results! \n\n\nIt highlighted my text like it was supposed to. But now the problem remains that I don't have all of the text classified, which I think is going to be a problem in the next exercise. I have a suspicion that the problem is that my computer isn't allocating enough memory, which the Stanford NER page says is a problem: \n\n\n\n\nIt might work to double-click on the stanford-ner.jar archive but this may well fail as the operating system does not give Java enough memory for our NER system, so it is safer to instead double click on the ner-gui.bat icon (Windows) or ner-gui.sh (Linux/Unix/MacOSX).\n\n\n\n\nI tried increasing the amount of memory Java gets by default, it did not work either. So now I'm going to try and run only half the file. If that works, I'll save the single file into two files, run each one individually, then amalgamate the two after. I know it doesn't exactly fix my issues long term but it is a working solution. \n\n\nSuccess with half the document! I've broken it into two individual files now to run through the NER gui.\n\n\nUpdate:\n\nThe splitting didn't work, it tagged the files but it only highlighted it in the gui. So I went back and edited the \nner-gui.bat\n file, adding a \npause\n function so I could read what the command file without it auto-quitting, I discovered it was having giving an \nunable to allocate space\n. SO I went into the .bat file and messed around with the \nJava -mx\n size, and found that it worked successfully, and I could run file through the gui and successfully save a tagged version of the file (which I couldn't do before). \n\n\nThe tagged Texas letters can be found \nhere\n.\n\n\nAnd the index can be found \nhere\n.", 
            "title": "Module 3"
        }, 
        {
            "location": "/Module_3/#module-3", 
            "text": "3/19/2016 1:16:47 PM", 
            "title": "Module 3"
        }, 
        {
            "location": "/Module_3/#text-encoding", 
            "text": "", 
            "title": "Text Encoding"
        }, 
        {
            "location": "/Module_3/#recovered-histories-vetting", 
            "text": "", 
            "title": "Recovered Histories Vetting"
        }, 
        {
            "location": "/Module_3/#negatives", 
            "text": "Site is \"Lottery Funded\", and does not appear to be affiliated with an academic institution  Websites design and style is outdated, which creates possibility that all the information is outdated, or not to modern standards   Does not have any outside links or sources, other than what is presented in its collection   A significant portion of the literature in the collection appears to be from slavers and free white men  Some items in collection have very messy handwriting, making it more difficult to encode the information and recognize characters", 
            "title": "Negatives:"
        }, 
        {
            "location": "/Module_3/#positives", 
            "text": "Site is \"Lottery Funded\", which can be viewed in a positive way, as it prevents people from becoming biased or for altering information to help prove their point  Encourages users to generate own content using site as raw resource  Appears to reference information in its collection by linking to the specific articles  Documents in collection are scanned and then uploaded, they are not transcribed, which can cause differences  Search tool provides what appears to be accurate reading of the collection", 
            "title": "Positives:"
        }, 
        {
            "location": "/Module_3/#transcription", 
            "text": "The transcription went smoothly at first. Upon entering my first tags, I tested it in Internet Explorer and got the colourfully transcribed version of  Negro Slavery by Zachary MacAulay . But then I went back and added some a few more place and name tags, and the text disappeared. I had the paragraphs separated and encased in tags, rather than inline tags, and I think that's where my problem came from. For example:  After going through each paragraph break, I managed to get proper formatting back, with colours. My suspicion is that one of my location tags got separated when I was spacing out the text. A simple mistake.", 
            "title": "Transcription"
        }, 
        {
            "location": "/Module_3/#transformations", 
            "text": "In my page5.xml file, the xsl styling is looking for tags like  persName  and  interp . These titles point the browser to the same tag in the .xsl document which defines the style of the .xml file. This uses the same system that web pages utilize, the HTML pointing the browser to the styles defined in the CSS document.   In  SG_transformer.xsl , the browser will be looking at tags such as  ID , or  Repository . I  believe  this uses an elif system, where it defines a list of words as the root of the tag, and each tag would receive a value-of, which might give a tooltip? I'm not 100% on the appearance of the .xml page once these tags are used.   To define different .xsl sheets, you would change the  href  designation on line 2 from:  ?cml-stylesheet type=\"text/xsl\" href=\"000stylesheet.xsl\"?  to  ?cml-stylesheet type=\"text/xsl\" href=\"SG_transformer.xsl\"? . I tried this and didn't get any change in my own .xml file, but I believe that is 1. because I don't have any of the tags and 2. because the  SG_transformer.xsl  file does not have any defined template styles in it. I compared it to the 000style.xsl and the colours and appearance of the tags are defined under the  style  tag throughout the document, which  SG_transformer.xsl  is lacking.   If you want to see the pages, you can view my transcription  here . I also apologize for putting the transformer.xsl file as code, the underscore in it was italicizing large swathes of my text that happened to fall between them.", 
            "title": "Transformations"
        }, 
        {
            "location": "/Module_3/#regex", 
            "text": "After reading the gentle introduction, I can understand how powerful it can be once one knows how to use it. It's a good thing I made a copy. I also need to be careful as to where I leave my cursor when I'm searching. When I leave it at the end, it's hard to find anything in the document after it.   \\r\\n  did not work to find the blank lines. To remove the white spaces, I went into  RegExr's Cheatsheet  and found the Regex for word, which was  \\w , and then used  \\r\\n[^\\w].+  to find all of the white spaces in my document to remove. It successfully worked, however it did leave a few large chunks with blank lines that I can't get RegEx to clear. I suspect that it is because there was more than one line with it, and when I replaced the previous characters with the the empty replace bar, it replaced everything, but maybe put in an invisible character I can't see. I'm not sure yet, still trying. For the sake of my sanity I had to go through and remove each chunk by hand. It wasn't difficult though, as there were only twelve or so of these chunks. I think it's where \"Digitized By Google\" used to sit in the document, and they may have put in something that my text editor can't see/copy. Still not sure.    I find that  RegExr  doesn't seem to process my work properly. I'm not sure why, but I actually find it relatively easy to use Notepad++ as it displays a window at the bottom of the document with all of the information that the find function works. And if something doesn't work, ctrl+Z works magical wonders.   While trying to replace the page number and comma at the end of each index entry, I was getting very bizarre numbers at the end of the entry. Until I realized I had put the space in the wrong spot. When replacing  (, )([0-9]{4})(.+)  with  \\2  I was getting an entry that looked like this:  ~Sam Houston to J. Pinckney Henderson, December 311836 \nThen I realized my misplaced space was in the wrong group, and found that  (,)( [0-9]{4})(.+)  worked significantly better. I proceeded to have a moment of panic when I noticed lines like this:  David G. Bumet, Alc6e La Branche, December 16, 1839 61\nDavid G. Bumet, Alc^ La Branche, December 17, 1839 63 \nBut then upon checking the original document, that is what Google's OCR thought the characters represented.  Something messed up my saves and I managed to save over my step 5 with an earlier version, so I have to backtrack a bit to get back on track, and redo step five and six. In the midst of finding this, I realized I had left the dates at the ends of each entry, so every search would come up with more than three commas in each sentence. So backtrack a little further to step 4. And I managed to fix it with the given Regex. I'm not a hundred percent on what made that occur.", 
            "title": "Regex"
        }, 
        {
            "location": "/Module_3/#open-refine", 
            "text": "Interesting tool. I worked through it all only to find that it left a few entries (possibly worse off). I think its because I might have been trying to add an accented e, but it didn't like it, as a few of my entries have an @ sign in it randomly that wasn't there before. I might try running it through again later, but at least I now know how to basically work the program (which is awesome). Just to play with it, I'm now uploading my .csv into Palladio to see what it comes up with. Data visualization is one of my favourite things!   I don't quite understand it at the moment. Maybe I'll come back to it.", 
            "title": "Open Refine"
        }, 
        {
            "location": "/Module_3/#stanford-ner", 
            "text": "So I'm having significant issues with this one. I can't seem to run the  ner-gui.bat  file to run, it opens a window only to immediately close it again. So following the directions on the end of the Open Refine page, I attempted to run it from the command line in the folder and I'm getting an exception in thread, but I can't figure out what I've done wrong. I have a feeling its something to do with a missing Java definition. I think it's specifically struggling with  edu.stanford.nlp.ie.crf.CRFClassifier     I figured it out. The problem was that I've been trying to run it from command line with the text from the  workbook , which wasn't working. So I tried updating Java, and did that successfully. Still nothing. I was able to successfully open a gui using  stanford-ner.jar , but when I put the text in and selected a classifier, it was not doing anything. So I deleted a large selection of it and finally got my results!   It highlighted my text like it was supposed to. But now the problem remains that I don't have all of the text classified, which I think is going to be a problem in the next exercise. I have a suspicion that the problem is that my computer isn't allocating enough memory, which the Stanford NER page says is a problem:    It might work to double-click on the stanford-ner.jar archive but this may well fail as the operating system does not give Java enough memory for our NER system, so it is safer to instead double click on the ner-gui.bat icon (Windows) or ner-gui.sh (Linux/Unix/MacOSX).   I tried increasing the amount of memory Java gets by default, it did not work either. So now I'm going to try and run only half the file. If that works, I'll save the single file into two files, run each one individually, then amalgamate the two after. I know it doesn't exactly fix my issues long term but it is a working solution.   Success with half the document! I've broken it into two individual files now to run through the NER gui.  Update: \nThe splitting didn't work, it tagged the files but it only highlighted it in the gui. So I went back and edited the  ner-gui.bat  file, adding a  pause  function so I could read what the command file without it auto-quitting, I discovered it was having giving an  unable to allocate space . SO I went into the .bat file and messed around with the  Java -mx  size, and found that it worked successfully, and I could run file through the gui and successfully save a tagged version of the file (which I couldn't do before).   The tagged Texas letters can be found  here .  And the index can be found  here .", 
            "title": "Stanford NER"
        }, 
        {
            "location": "/Module_4/", 
            "text": "Module 4\n\n\n4/5/2016 2:34:53 PM \n\n\nSeeing Patterns\n\n\nSo I'm late, I know. I shouldn't complain about anything, but I am getting the work done, and I do hope *crosses fingers* that I'll be able to get it all done before April 8th. I do like a challenge. The concept of data wrangling is such a cool and important subject. With each new addition to the digital collection of human thought, piles of data are becoming more and more overwhelming. In \nmodule 3\n we fixed and cleaned up the data from the \nTexas Correspondence\n, and now we're looking at representing this data. \n\n\nThe first thing examined in this module is several projects that work to model data, a very interesting and (relatively new) work that uses history's new macro perspective. For example, in the \nMapping Texts\n project, the teams at Stanford University and the University of North Texas worked together to create \n\n\n\n\nInteractive models that would\nexperiment with methods for combining text-mining with visualizations, using text-mining to\ndiscover meaningful language patterns in large-scale text collections and then employ\nvisualizations in order to make sense of them. (\nMapping Texts, page 6\n)\n\n\n\n\nThis project, aside from being \nincredibly cool\n is also important, as it allows people to visually see represented information that has been mined from a text. It's not necessarily a first, but to be able to see how many times Dallas appears in the newspapers, and to see that represented larger on a map of Texas. The analysis of large data sets is such a useful tool. And when the OCR text is accurate (which appears to have been a problem for the teams as well, as they addressed it in their document), it allows for a more accurate and faster analysis of the documents. The usefulness of this is not lost on me. To be able to look at a body of texts and to identify and place the areas that they mention is incredible, as it gives people an easier way to recognize data. \n\n\nThe next piece examined is the blog post \nCorpus Lingustics for Historians\n discusses the computer analysis of relationships between words, which I \nbelieve\n allows computers to find the general context of certain words and phrases throughout a body of works, and can give a general impression of themes. This can effectively allow someone to analyze the use of words, density, placement, and other correlations that I might not even be able to think about yet! In \nVisualizing Gender in History of Woman Suffrage\n, Michelle Moravec creates correlations that examine the \nfeelings\n towards certain words, such as that of \nmy husband\n which appears in mostly negative terms. These correlations that reveal information that would otherwise require a human to read through and make the correlations themselves (which would be significantly more difficult and full of errors) is just amazing. \n\n\nReturn to the podcast\n \n\n\nNetwork analysis on \nscottbot.net\n gives several examples of network examples and metrics and bimodality. I've read some of it, and honestly don't entirely understand what network analysis is outside of connecting related objects and analyzing said connections. Topic modelling is also another beautiful way to display information and (hehe) networking (is that right?). Topic modelling allows connections people might not have noticed before become apparent, and shows the relative values of it. \n\n\nExercises\n\n\nGephi\n\n\nSo upon opening Gephi, the first thing I did was attempt to import the .csv file from the previous exercise. I did as the instruction said, but was getting the error that there were empty columns. So I had to open up the csv file and find what the one blank cell that happened to be causing me troubles. I managed to find it though. I'm noticing however, that every column has a weight of 1, which is incorrect. They should be dependent on how many letters are sent by the same people. I've also noticed that I have slightly more edges than I'm supposed to (approximately 588 more). I'm not sure what happened, but I think that my csv sheet has been majorly screwed up and I'm not a hundred percent sure why, as everything appears as it should. \n\n\nSo in an attempt to clear things up I ran it through open refine again and fixed some of the stranger characters, still nothing. I set a column to specify an undirected network which Gephi liked - at first. I entered the data again and ran the Connected Components algorithm and got the appropriate results, but I still haven't gotten the appropriate number of edges or a differing weight value. Moving on from that, when I run PageRank I find another difficulty. I'm supposed to specify a directed network now, which is something I can't do because of specifying it as directed in the csv sheet. And there appears to be no way to remove the column, so I'm again going to have to re-add the sheet. \n\n\nLuckily adding the same sheet again and again makes a lot of the instructions pretty easy to replicate. A few minutes later I'm back to where I was when I started. Everything has worked out from this point. In the ranking area I had a slight difficulty, the instructions differed, I could not find a 'ranking' tab or a red diamond. So I used the size selection and used PageRank to adjust the sizes, which worked just the same (I hope). \n\n\nUpon previewing it, I found that my network was weirdly sideways? To put it plainly. The \nworkbook\n says that the communities should be upper and lower, which mine seem to be more of a left-right distribution, but I suppose it still represents the information in a relatively similar form. The two communities are still similarly discernable (although I think there could be a way to distribute it better and represent the information easier). To get the weight working would probably be relatively useful. However, the distribution is still interesting to see, such as how Ashbel Smith and Anson Jones dominate the community, as it was likely during the time of Anson Jones that the letters were documented. Chances are that Sam Houston, who barely appears in the network, his letters were either lost, or not accounted for as he was no longer president and was not sending official correspondence. \n\n\nThen I broke my upload notebook by trying to rename it from reedhaw.github.io to dighital-history. I need to stop messing around with what's working. But now it's back up and running.\n\n\nTopic Modelling\n\n\nThis was relatively straightforward. I had no hiccups (surprisingly, as my machine can never seem to just \ndo\n something as expected). I can see and understand how the topic modelling \nmight\n be useful, but I'm not a hundred percent on how it exactly helps. I see a list of topics, but the \"topics\" are just a line of random words that seem to have a similar meaning. ship letter board kingdom arrived young extract captain dated passenger doesn't mean a whole lot to me, but by using it to find the selection of words in each of the documents. I can understand how one would use it to view distribution of words throughout docs, and to easily see correlations between docs. My interpretation of this program is that it best estimates what the topic of each of the document is, and generates a list to show you the number of times the words appear in each document. Or along those lines. By importing the csvs into a network modelling program, you could create visual correlations of specific word usage in each document, or something along those lines. \n\n\nGoing further, I ran my files from the Canadiana API and the topics all came up as html work (text, type, cookie, script, meta, body, etc.). I'm not actually a hundred percent certain I ran the right files but that's what I got!\n\n\nTopic Modelling in R\n\n\nI absolutely love the style of \ncodeschool.com\n. It also lays out everything in pretty straightforward terms. I completed all seven chapters of the Code School tutorial. \n\n\n\n\nSo moving onto the next stage, I am faced with RStudio, and the first problem I run into is that I can't load the rJava library. So I'm scrolling through forums and I find people talking about adding the Java location to my \nPATH\n variable, which I do. No success. The next thing I find is that I need R and Java to run on the same architecture (64 bit in my case), which I discovered I was using the Java Development Kit in 32 bit in the most modern update, and 64 bit in a slightly older version. I am currently attempting to update to see if that resolves my problem. This solved my problem. \n\n\nFollowing the tutorial, I had no problems with this, and it all went smoothly. Check my \nGitHub repository\n for my script of this (among other things). \n\n\nText Analysis with Overview\n\n\nOverview makes a ton of sense to me, and is an analysis program that makes sense. Uploading the CND.csv however did not give me an option to choose words. The use of systems similar to this is incredibly interesting, as it allows for an unparalleled examination of keywords and allows for rapid navigation of large text bodies. It is actually very similar to the topic modelling software used in exercise two. \n\n\nAntConc\n\n\nWorking with AntConc is an incredible piece of software for analysis (much like the former few) but I feel that these features are ones that I can truly relate to, and something that I could use in my own research to identify both the context and the specific information provided. \n\n\nIn entering the data from the CND Dataset, I had to first split the file. After a few seconds of browsing Google I found a tool called CSV splitter, a program that did exactly what its name denotes. After setting the parameters to one row per file, I suddenly see 357 new files in my folder, which is exactly what I wanted (for once). And I immediately spoke too soon. They were all exported as csv when I need tab deliminated files instead. Achieved this by using command prompt and running this code: \nren\n *.csv *.txt\n. It worked perfectly, and although it might not quite be the proper way to go about it, it certainly worked for my purposes. These split files were successfully opened in AntConc!\n\n\nText Analysis with Voyant\n\n\nWhat an interesting tool. The very act of uploading the corpus in chronological order changes the data representation significantly. It goes from most common words of \"the, and, by , to, of\" to \"new, mr, country, government, great\". This shift (and the increase in colours) gives a better representation of what is displayed. And I just found out the hard way that markdown pad does NOT like when really long HTML code is thrown into it. \nlooking at you Voyant Tools HTML 'snippet'\n. By applying stop words I see (in the original unordered list) the words I took note of before disappear, and are now showing em \"united, scotland, kingdom, mr\" etc. Removing stop words is essential (and essential to have an accurate list) as they will skew word results and data that could be unimportant (such as titles, headers, metadata). \n\n\nQuick Charts Using RAW\n\n\nThe only problem I've had thus far with RAW had nothing to do with it, rather it was the problem with the spreadsheet, the blank detector simply did not want to cooperate, but I worked away until it agreed to do what I wanted. So far, what I can say about RAW is that it is beautifully designed.\n\n\nUpon placing Place 1 and Place 2 into the system, Texas and Mexico immediately popped out as the largest places documentation went to. Place 3 and Place 4 immediately appear the same way. In the second set, there appeared to be a significant amount of correspondence being sent to the same place, whereas the first set the correspondence seemed relatively mixed. \n\n\nGeorectifying\n\n\nGoogle tiles warped map\n A relatively straight forward exercise, I found it easiest to line the control points up with that of streets, and attempt at some geographic features. The biggest problem was that the map I happened to pick turned out to have a large cut out in the middle, displaying what appeared to be a different area that did not quite line up with that of Buckingham, Quebec. Took me a while to finally find the small town of Buckingham, Quebec. But I finally did it and it successfully uploaded to Palladio with no troubles. \n\n\nText Analysis with R\n\n\nI thing that I might have removed a little too much in this exercise, as the plots give two words now, \"will\" and \"idglasgow\" in the word cloud. So something has gone wrong, and I'm not entirely sure of what yet, but I will attempt to rectify this later (as I am now in quite the time crunch). \n\n\nFor exercise 10, both github links don't seem to work, and displayed a 404 error. So I'm going to go ahead and move on to Module 5.", 
            "title": "Module 4"
        }, 
        {
            "location": "/Module_4/#module-4", 
            "text": "4/5/2016 2:34:53 PM", 
            "title": "Module 4"
        }, 
        {
            "location": "/Module_4/#seeing-patterns", 
            "text": "So I'm late, I know. I shouldn't complain about anything, but I am getting the work done, and I do hope *crosses fingers* that I'll be able to get it all done before April 8th. I do like a challenge. The concept of data wrangling is such a cool and important subject. With each new addition to the digital collection of human thought, piles of data are becoming more and more overwhelming. In  module 3  we fixed and cleaned up the data from the  Texas Correspondence , and now we're looking at representing this data.   The first thing examined in this module is several projects that work to model data, a very interesting and (relatively new) work that uses history's new macro perspective. For example, in the  Mapping Texts  project, the teams at Stanford University and the University of North Texas worked together to create    Interactive models that would\nexperiment with methods for combining text-mining with visualizations, using text-mining to\ndiscover meaningful language patterns in large-scale text collections and then employ\nvisualizations in order to make sense of them. ( Mapping Texts, page 6 )   This project, aside from being  incredibly cool  is also important, as it allows people to visually see represented information that has been mined from a text. It's not necessarily a first, but to be able to see how many times Dallas appears in the newspapers, and to see that represented larger on a map of Texas. The analysis of large data sets is such a useful tool. And when the OCR text is accurate (which appears to have been a problem for the teams as well, as they addressed it in their document), it allows for a more accurate and faster analysis of the documents. The usefulness of this is not lost on me. To be able to look at a body of texts and to identify and place the areas that they mention is incredible, as it gives people an easier way to recognize data.   The next piece examined is the blog post  Corpus Lingustics for Historians  discusses the computer analysis of relationships between words, which I  believe  allows computers to find the general context of certain words and phrases throughout a body of works, and can give a general impression of themes. This can effectively allow someone to analyze the use of words, density, placement, and other correlations that I might not even be able to think about yet! In  Visualizing Gender in History of Woman Suffrage , Michelle Moravec creates correlations that examine the  feelings  towards certain words, such as that of  my husband  which appears in mostly negative terms. These correlations that reveal information that would otherwise require a human to read through and make the correlations themselves (which would be significantly more difficult and full of errors) is just amazing.   Return to the podcast    Network analysis on  scottbot.net  gives several examples of network examples and metrics and bimodality. I've read some of it, and honestly don't entirely understand what network analysis is outside of connecting related objects and analyzing said connections. Topic modelling is also another beautiful way to display information and (hehe) networking (is that right?). Topic modelling allows connections people might not have noticed before become apparent, and shows the relative values of it.", 
            "title": "Seeing Patterns"
        }, 
        {
            "location": "/Module_4/#exercises", 
            "text": "", 
            "title": "Exercises"
        }, 
        {
            "location": "/Module_4/#gephi", 
            "text": "So upon opening Gephi, the first thing I did was attempt to import the .csv file from the previous exercise. I did as the instruction said, but was getting the error that there were empty columns. So I had to open up the csv file and find what the one blank cell that happened to be causing me troubles. I managed to find it though. I'm noticing however, that every column has a weight of 1, which is incorrect. They should be dependent on how many letters are sent by the same people. I've also noticed that I have slightly more edges than I'm supposed to (approximately 588 more). I'm not sure what happened, but I think that my csv sheet has been majorly screwed up and I'm not a hundred percent sure why, as everything appears as it should.   So in an attempt to clear things up I ran it through open refine again and fixed some of the stranger characters, still nothing. I set a column to specify an undirected network which Gephi liked - at first. I entered the data again and ran the Connected Components algorithm and got the appropriate results, but I still haven't gotten the appropriate number of edges or a differing weight value. Moving on from that, when I run PageRank I find another difficulty. I'm supposed to specify a directed network now, which is something I can't do because of specifying it as directed in the csv sheet. And there appears to be no way to remove the column, so I'm again going to have to re-add the sheet.   Luckily adding the same sheet again and again makes a lot of the instructions pretty easy to replicate. A few minutes later I'm back to where I was when I started. Everything has worked out from this point. In the ranking area I had a slight difficulty, the instructions differed, I could not find a 'ranking' tab or a red diamond. So I used the size selection and used PageRank to adjust the sizes, which worked just the same (I hope).   Upon previewing it, I found that my network was weirdly sideways? To put it plainly. The  workbook  says that the communities should be upper and lower, which mine seem to be more of a left-right distribution, but I suppose it still represents the information in a relatively similar form. The two communities are still similarly discernable (although I think there could be a way to distribute it better and represent the information easier). To get the weight working would probably be relatively useful. However, the distribution is still interesting to see, such as how Ashbel Smith and Anson Jones dominate the community, as it was likely during the time of Anson Jones that the letters were documented. Chances are that Sam Houston, who barely appears in the network, his letters were either lost, or not accounted for as he was no longer president and was not sending official correspondence.   Then I broke my upload notebook by trying to rename it from reedhaw.github.io to dighital-history. I need to stop messing around with what's working. But now it's back up and running.", 
            "title": "Gephi"
        }, 
        {
            "location": "/Module_4/#topic-modelling", 
            "text": "This was relatively straightforward. I had no hiccups (surprisingly, as my machine can never seem to just  do  something as expected). I can see and understand how the topic modelling  might  be useful, but I'm not a hundred percent on how it exactly helps. I see a list of topics, but the \"topics\" are just a line of random words that seem to have a similar meaning. ship letter board kingdom arrived young extract captain dated passenger doesn't mean a whole lot to me, but by using it to find the selection of words in each of the documents. I can understand how one would use it to view distribution of words throughout docs, and to easily see correlations between docs. My interpretation of this program is that it best estimates what the topic of each of the document is, and generates a list to show you the number of times the words appear in each document. Or along those lines. By importing the csvs into a network modelling program, you could create visual correlations of specific word usage in each document, or something along those lines.   Going further, I ran my files from the Canadiana API and the topics all came up as html work (text, type, cookie, script, meta, body, etc.). I'm not actually a hundred percent certain I ran the right files but that's what I got!", 
            "title": "Topic Modelling"
        }, 
        {
            "location": "/Module_4/#topic-modelling-in-r", 
            "text": "I absolutely love the style of  codeschool.com . It also lays out everything in pretty straightforward terms. I completed all seven chapters of the Code School tutorial.    So moving onto the next stage, I am faced with RStudio, and the first problem I run into is that I can't load the rJava library. So I'm scrolling through forums and I find people talking about adding the Java location to my  PATH  variable, which I do. No success. The next thing I find is that I need R and Java to run on the same architecture (64 bit in my case), which I discovered I was using the Java Development Kit in 32 bit in the most modern update, and 64 bit in a slightly older version. I am currently attempting to update to see if that resolves my problem. This solved my problem.   Following the tutorial, I had no problems with this, and it all went smoothly. Check my  GitHub repository  for my script of this (among other things).", 
            "title": "Topic Modelling in R"
        }, 
        {
            "location": "/Module_4/#text-analysis-with-overview", 
            "text": "Overview makes a ton of sense to me, and is an analysis program that makes sense. Uploading the CND.csv however did not give me an option to choose words. The use of systems similar to this is incredibly interesting, as it allows for an unparalleled examination of keywords and allows for rapid navigation of large text bodies. It is actually very similar to the topic modelling software used in exercise two.", 
            "title": "Text Analysis with Overview"
        }, 
        {
            "location": "/Module_4/#antconc", 
            "text": "Working with AntConc is an incredible piece of software for analysis (much like the former few) but I feel that these features are ones that I can truly relate to, and something that I could use in my own research to identify both the context and the specific information provided.   In entering the data from the CND Dataset, I had to first split the file. After a few seconds of browsing Google I found a tool called CSV splitter, a program that did exactly what its name denotes. After setting the parameters to one row per file, I suddenly see 357 new files in my folder, which is exactly what I wanted (for once). And I immediately spoke too soon. They were all exported as csv when I need tab deliminated files instead. Achieved this by using command prompt and running this code:  ren\n *.csv *.txt . It worked perfectly, and although it might not quite be the proper way to go about it, it certainly worked for my purposes. These split files were successfully opened in AntConc!", 
            "title": "AntConc"
        }, 
        {
            "location": "/Module_4/#text-analysis-with-voyant", 
            "text": "What an interesting tool. The very act of uploading the corpus in chronological order changes the data representation significantly. It goes from most common words of \"the, and, by , to, of\" to \"new, mr, country, government, great\". This shift (and the increase in colours) gives a better representation of what is displayed. And I just found out the hard way that markdown pad does NOT like when really long HTML code is thrown into it.  looking at you Voyant Tools HTML 'snippet' . By applying stop words I see (in the original unordered list) the words I took note of before disappear, and are now showing em \"united, scotland, kingdom, mr\" etc. Removing stop words is essential (and essential to have an accurate list) as they will skew word results and data that could be unimportant (such as titles, headers, metadata).", 
            "title": "Text Analysis with Voyant"
        }, 
        {
            "location": "/Module_4/#quick-charts-using-raw", 
            "text": "The only problem I've had thus far with RAW had nothing to do with it, rather it was the problem with the spreadsheet, the blank detector simply did not want to cooperate, but I worked away until it agreed to do what I wanted. So far, what I can say about RAW is that it is beautifully designed.  Upon placing Place 1 and Place 2 into the system, Texas and Mexico immediately popped out as the largest places documentation went to. Place 3 and Place 4 immediately appear the same way. In the second set, there appeared to be a significant amount of correspondence being sent to the same place, whereas the first set the correspondence seemed relatively mixed.", 
            "title": "Quick Charts Using RAW"
        }, 
        {
            "location": "/Module_4/#georectifying", 
            "text": "Google tiles warped map  A relatively straight forward exercise, I found it easiest to line the control points up with that of streets, and attempt at some geographic features. The biggest problem was that the map I happened to pick turned out to have a large cut out in the middle, displaying what appeared to be a different area that did not quite line up with that of Buckingham, Quebec. Took me a while to finally find the small town of Buckingham, Quebec. But I finally did it and it successfully uploaded to Palladio with no troubles.", 
            "title": "Georectifying"
        }, 
        {
            "location": "/Module_4/#text-analysis-with-r", 
            "text": "I thing that I might have removed a little too much in this exercise, as the plots give two words now, \"will\" and \"idglasgow\" in the word cloud. So something has gone wrong, and I'm not entirely sure of what yet, but I will attempt to rectify this later (as I am now in quite the time crunch).   For exercise 10, both github links don't seem to work, and displayed a 404 error. So I'm going to go ahead and move on to Module 5.", 
            "title": "Text Analysis with R"
        }, 
        {
            "location": "/Module_5/", 
            "text": "Module 5\n\n\nThis module I read through, and I'm not going to lie, I skipped. I didn't do this because I thought I was better than that, but more because I have a very solid grasp on communication and colour. I have used Photoshop and many other visual programs, and honestly I've fallen behind in this course. I hope that my blog and web page designs are enough to work as proof of my competency in this, as they are things that I have worked hard to get to look the way I want them to (sometimes for unnecessarily long amounts of time). \n\n\nAnd in the process of reading this, many of the exercises I am going to encorporate into my final project website, which will be hosted alongside my regular domain at \nreedhaw.ca/final\n (I hope, still working with Reclaim Hosting to rectify \n.htaccess\n errors and redirect loops). In the meantime, if you want to see some visual work on history I have done lately, you can check out the video I (hastily) edited for a friend below: \n\n\nhttps://drive.google.com/open?id=0B97p5RZhvFrzRi1ZOWttUHEyZ2c", 
            "title": "Module 5"
        }, 
        {
            "location": "/Module_5/#module-5", 
            "text": "This module I read through, and I'm not going to lie, I skipped. I didn't do this because I thought I was better than that, but more because I have a very solid grasp on communication and colour. I have used Photoshop and many other visual programs, and honestly I've fallen behind in this course. I hope that my blog and web page designs are enough to work as proof of my competency in this, as they are things that I have worked hard to get to look the way I want them to (sometimes for unnecessarily long amounts of time).   And in the process of reading this, many of the exercises I am going to encorporate into my final project website, which will be hosted alongside my regular domain at  reedhaw.ca/final  (I hope, still working with Reclaim Hosting to rectify  .htaccess  errors and redirect loops). In the meantime, if you want to see some visual work on history I have done lately, you can check out the video I (hastily) edited for a friend below:   https://drive.google.com/open?id=0B97p5RZhvFrzRi1ZOWttUHEyZ2c", 
            "title": "Module 5"
        }, 
        {
            "location": "/Overview/", 
            "text": "Final Project\n\n\nOverview\n\n\nFor my final project I've decided to work with the Colonial Newspapers Database (CND), which contains extracts from letters and newspapers (from what I can tell upon first inspection). The information is easier to work with, as I have used it previously in several other Modules, and contains information including year, place and information written from that time period. There appears to be 384 unique segments, each identified at the start with ID*. \n\n\nThe first task, I will run it through Voyant tools to identify trends and information to base a question off, and I will use this one because I like the graphic interface and information it provides. I recognize that Dr. Graham has already run this same database through the tool and organized the information by uploading each document and treated in chronological order. This upload can be found \nhere\n. I will be using my own upload however, so I can see individual settings. \n\n\nBefore uploading it to Voyant Tools, I've opened it in Excel to get a better understanding of the information in the document. I immediately sorted the data by year, and saved it as a separate file. I then split that file by using csv splitter, and batch renaming the files to .txt files, and uploaded the files to Voyant tools, allowing me to view the newspaper documents in chronological order. This provided an overview of the provided documents. \n\n\nThe largest amount of words provided in this sucker tended to be ones like Scotland, united, Glasgow and the likes. This is because the corpus lists the location of each of the newspapers, and the newspaper itself, which doesn't really help me. So the next thing I tried was to edit out the newspaper name and location to get a better understanding of the actual words inside each entry. \n\n\nI repeated the same process as the above one and now have a list of documents that only contains the year, the ID number and the text of the document. By removing the keywords, the newspaper name, location and such, it will give me a clear representation of the actual information that is still sorted chronologically. \n\n\nThis gave me a significantly better representation of the data. The leading terms in it include country, new, Mr, great and states. I added the stop word Mr, as most letters are likely to address people as Mr so-and-so, which is still no use to me. The resulting Word cloud can be found \nhere\n\n\nThis leaves me with these as the top ten terms in the corpus: \n\n\n\n\nnew\n\n\ncountry\n\n\ngreat\n\n\nstates\n\n\ntime\n\n\nunited\n\n\nstate\n\n\ngovernment\n\n\ngeneral\n\n\nland\n\n\n\n\nSo what can I do with this? I looked at the links, which looks at a word's context. In examining country, great and new are both largely associated with it, but I also find the terms south, Britain, Wales, States, and York. In adjusting the context, I find that the words surrounding country most often relate to the expansionism of the colonial period, for example:\n\n\n\n\ncolony\n\n\ngrant\n\n\nland\n\n\nstates\n\n\nemigration\n\n\nindians\n\n\n\n\nThis could imply the interest of these regions (Britain, Scotland, and their subsidiary colonies) in colonial life, and the importance of these areas in regards to the central focus to the homeland/necessity of the colony's resources. \n\n\nThere are a number of things I can examine with this information. Because it is laid out in chronological order, I could examine the usage of words over time, and how often it is used and its context. The unfortunate part is that voyant tools only examines 100 segments at a time, and there doesn't seem to be any way of shifting the data over to see the rest of it. Another option is I could examine the number of papers to come out at a time from a certain area in regards to word usage, length of paper, etc. The date range of the collection is from January 5th, 1789 to May 18, 1842. \n\n\nIdea: Examine the 'type' of article listed by the database (inform, persuade, advertisment, or entertain), and the general distribution of each type of article through time. Then examine the tendencies of word distribution in each category to get a further understanding of the information in each category. The question: What are the general trends of information in each category of article in the database, and how do they change over time? \n\n\nBeginning Process\n\n\nDivide each category type into individual csv, then sort them and divide them based on each filter, etc. The breakdown of each filetype can be seen below:\n\n\n\n\n\n\n\n\nType\n\n\nNumber\n\n\nPercent\n\n\n\n\n\n\n\n\n\n\nAdvertisements\n\n\n2\n\n\n0.54%\n\n\n\n\n\n\nEntertainment\n\n\n37\n\n\n10.03%\n\n\n\n\n\n\nInformative\n\n\n313\n\n\n84.82\n\n\n\n\n\n\nPersuade\n\n\n17\n\n\n4.60 %\n\n\n\n\n\n\nNew\n\n\n1\n\n\n0.27%\n\n\n\n\n\n\n\n\nThis division is interesting, and upon feeding each of these types into Voyant tools, I got a Cirrus showing me what words are most often used in each instance. Below I will examine each of the Cirrus word clouds to figure out what each type of article refers mostly to. \n\n\nAdverisments\n\n\nThere were only two advertisements, making up only 0.5% of the total corpus, and in the two articles, the top words are as follows: \n- mills\n- Kentucky\n- acre\n- state \n- sugar\nThis distribution is incredibly skewed, however, as the first of the advertisements (ID35) from December 1789 is only 37 characters long, whereas the second advertisement (ID368) from 1794 is 497 characters long, and originates from Kentucky, where its selling land. \n\n\nhttp://voyant-tools.org/?corpus=2e6a910b1151e7fb9c042dbc7d79e4e1\n\n\nEntertainment\n\n\nEntertainment makes up 10% of the corpus with 37 articles. The top words are as follows: \n- people\n- chief\n- king\n- natives \n- man\nThese articles refer to entertaining materials, but there is an interesting spike, with all five of the top words being used significantly more for a longer and more steady time from the article (ID358) with the heading \"Dreadful Massacre by the Natives of the Marquesas Islands\", and then another article (ID362) with the heading \"Another Dreadful Massacre by the Natives of the Marquesas Islands\". These two events single handed bumped up the occurrence of all of these words for a period. \n\n\nhttp://voyant-tools.org/?corpus=dd900a9396b0d303045f5511319c7b52\n\n\nInformative\n\n\nInformative articles made up 84.8% of the corpus, and existed in 313 articles. The largest category, as information transfer was primarily done through newspapers (hence the \nnews\n part). The top words are as follows: \n- new\n- country\n- states\n- great\n- united\nFrom simply examining this one can see a overwhelming lean towards nationalistic information, as news focused on political nature. Voyant tools is not the best tool for handling this information, as the graphs only show information on a select number of documents at one time (100 in the Trends graph), and I will have to pull it into another program to rework. \n\n\nhttp://voyant-tools.org/?corpus=d966b430e7fa34a9b0cbd68f9a0bfae4\n\n\nPersuasive\n\n\nPersuasive articles made up 4.6% of the corpus, with 17 articles flagged under it. These articles had a number of implications, mostly that of influencing people to move (similar to the advertisement article), and the top words are as follows: \n- country\n- states\n- great\n- new\n- American\nThe very composition of these words implies a few things: the 'states', great, new, American. To persuade people to move, these are the types of words one could expect to find in a new article, especially important when considering that this was during a time when it was vital to bring in new immigrants to establish the colonies. Emigration appears 23 times in the 17 articles, and society appears 26 times, labour 25 times. \n\n\nhttp://voyant-tools.org/?corpus=ec376fe5d3ffa64ce3f68b6294ff9c43\n\n\nNew\n\n\nThe new tag seems to be almost a mistake, and I have copied the contents of the soul article below, in csv format:\n\n\n\n\nID56   Glasgow Advertiser  Glasgow Scotland    United Kingdom  1789    12  7   new This morning some dispatches were received at the Secretary of State's Office from Halifax which were brought over in the Friendship Capt. Taylor arrived at Liverpool.\u00c2\u00a0   post\n\n\n\n\nThe purpose of this 'post' is unknown, aside to allow people to know that Taylor had arrived in Liverpool.", 
            "title": "Overview"
        }, 
        {
            "location": "/Overview/#final-project", 
            "text": "", 
            "title": "Final Project"
        }, 
        {
            "location": "/Overview/#overview", 
            "text": "For my final project I've decided to work with the Colonial Newspapers Database (CND), which contains extracts from letters and newspapers (from what I can tell upon first inspection). The information is easier to work with, as I have used it previously in several other Modules, and contains information including year, place and information written from that time period. There appears to be 384 unique segments, each identified at the start with ID*.   The first task, I will run it through Voyant tools to identify trends and information to base a question off, and I will use this one because I like the graphic interface and information it provides. I recognize that Dr. Graham has already run this same database through the tool and organized the information by uploading each document and treated in chronological order. This upload can be found  here . I will be using my own upload however, so I can see individual settings.   Before uploading it to Voyant Tools, I've opened it in Excel to get a better understanding of the information in the document. I immediately sorted the data by year, and saved it as a separate file. I then split that file by using csv splitter, and batch renaming the files to .txt files, and uploaded the files to Voyant tools, allowing me to view the newspaper documents in chronological order. This provided an overview of the provided documents.   The largest amount of words provided in this sucker tended to be ones like Scotland, united, Glasgow and the likes. This is because the corpus lists the location of each of the newspapers, and the newspaper itself, which doesn't really help me. So the next thing I tried was to edit out the newspaper name and location to get a better understanding of the actual words inside each entry.   I repeated the same process as the above one and now have a list of documents that only contains the year, the ID number and the text of the document. By removing the keywords, the newspaper name, location and such, it will give me a clear representation of the actual information that is still sorted chronologically.   This gave me a significantly better representation of the data. The leading terms in it include country, new, Mr, great and states. I added the stop word Mr, as most letters are likely to address people as Mr so-and-so, which is still no use to me. The resulting Word cloud can be found  here  This leaves me with these as the top ten terms in the corpus:    new  country  great  states  time  united  state  government  general  land   So what can I do with this? I looked at the links, which looks at a word's context. In examining country, great and new are both largely associated with it, but I also find the terms south, Britain, Wales, States, and York. In adjusting the context, I find that the words surrounding country most often relate to the expansionism of the colonial period, for example:   colony  grant  land  states  emigration  indians   This could imply the interest of these regions (Britain, Scotland, and their subsidiary colonies) in colonial life, and the importance of these areas in regards to the central focus to the homeland/necessity of the colony's resources.   There are a number of things I can examine with this information. Because it is laid out in chronological order, I could examine the usage of words over time, and how often it is used and its context. The unfortunate part is that voyant tools only examines 100 segments at a time, and there doesn't seem to be any way of shifting the data over to see the rest of it. Another option is I could examine the number of papers to come out at a time from a certain area in regards to word usage, length of paper, etc. The date range of the collection is from January 5th, 1789 to May 18, 1842.   Idea: Examine the 'type' of article listed by the database (inform, persuade, advertisment, or entertain), and the general distribution of each type of article through time. Then examine the tendencies of word distribution in each category to get a further understanding of the information in each category. The question: What are the general trends of information in each category of article in the database, and how do they change over time?", 
            "title": "Overview"
        }, 
        {
            "location": "/Overview/#beginning-process", 
            "text": "Divide each category type into individual csv, then sort them and divide them based on each filter, etc. The breakdown of each filetype can be seen below:     Type  Number  Percent      Advertisements  2  0.54%    Entertainment  37  10.03%    Informative  313  84.82    Persuade  17  4.60 %    New  1  0.27%     This division is interesting, and upon feeding each of these types into Voyant tools, I got a Cirrus showing me what words are most often used in each instance. Below I will examine each of the Cirrus word clouds to figure out what each type of article refers mostly to.", 
            "title": "Beginning Process"
        }, 
        {
            "location": "/Overview/#adverisments", 
            "text": "There were only two advertisements, making up only 0.5% of the total corpus, and in the two articles, the top words are as follows: \n- mills\n- Kentucky\n- acre\n- state \n- sugar\nThis distribution is incredibly skewed, however, as the first of the advertisements (ID35) from December 1789 is only 37 characters long, whereas the second advertisement (ID368) from 1794 is 497 characters long, and originates from Kentucky, where its selling land.   http://voyant-tools.org/?corpus=2e6a910b1151e7fb9c042dbc7d79e4e1", 
            "title": "Adverisments"
        }, 
        {
            "location": "/Overview/#entertainment", 
            "text": "Entertainment makes up 10% of the corpus with 37 articles. The top words are as follows: \n- people\n- chief\n- king\n- natives \n- man\nThese articles refer to entertaining materials, but there is an interesting spike, with all five of the top words being used significantly more for a longer and more steady time from the article (ID358) with the heading \"Dreadful Massacre by the Natives of the Marquesas Islands\", and then another article (ID362) with the heading \"Another Dreadful Massacre by the Natives of the Marquesas Islands\". These two events single handed bumped up the occurrence of all of these words for a period.   http://voyant-tools.org/?corpus=dd900a9396b0d303045f5511319c7b52", 
            "title": "Entertainment"
        }, 
        {
            "location": "/Overview/#informative", 
            "text": "Informative articles made up 84.8% of the corpus, and existed in 313 articles. The largest category, as information transfer was primarily done through newspapers (hence the  news  part). The top words are as follows: \n- new\n- country\n- states\n- great\n- united\nFrom simply examining this one can see a overwhelming lean towards nationalistic information, as news focused on political nature. Voyant tools is not the best tool for handling this information, as the graphs only show information on a select number of documents at one time (100 in the Trends graph), and I will have to pull it into another program to rework.   http://voyant-tools.org/?corpus=d966b430e7fa34a9b0cbd68f9a0bfae4", 
            "title": "Informative"
        }, 
        {
            "location": "/Overview/#persuasive", 
            "text": "Persuasive articles made up 4.6% of the corpus, with 17 articles flagged under it. These articles had a number of implications, mostly that of influencing people to move (similar to the advertisement article), and the top words are as follows: \n- country\n- states\n- great\n- new\n- American\nThe very composition of these words implies a few things: the 'states', great, new, American. To persuade people to move, these are the types of words one could expect to find in a new article, especially important when considering that this was during a time when it was vital to bring in new immigrants to establish the colonies. Emigration appears 23 times in the 17 articles, and society appears 26 times, labour 25 times.   http://voyant-tools.org/?corpus=ec376fe5d3ffa64ce3f68b6294ff9c43", 
            "title": "Persuasive"
        }, 
        {
            "location": "/Overview/#new", 
            "text": "The new tag seems to be almost a mistake, and I have copied the contents of the soul article below, in csv format:   ID56   Glasgow Advertiser  Glasgow Scotland    United Kingdom  1789    12  7   new This morning some dispatches were received at the Secretary of State's Office from Halifax which were brought over in the Friendship Capt. Taylor arrived at Liverpool.\u00c2\u00a0   post   The purpose of this 'post' is unknown, aside to allow people to know that Taylor had arrived in Liverpool.", 
            "title": "New"
        }
    ]
}